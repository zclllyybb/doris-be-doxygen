\hypertarget{memcpy__x86__64_8cpp}{}\doxysection{/\+Users/dabowang/doris/be/src/glibc-\/compatibility/memcpy/memcpy\+\_\+x86\+\_\+64.cpp 文件参考}
\label{memcpy__x86__64_8cpp}\index{/Users/dabowang/doris/be/src/glibc-\/compatibility/memcpy/memcpy\_x86\_64.cpp@{/Users/dabowang/doris/be/src/glibc-\/compatibility/memcpy/memcpy\_x86\_64.cpp}}
{\ttfamily \#include $<$stddef.\+h$>$}\newline
{\ttfamily \#include $<$emmintrin.\+h$>$}\newline
memcpy\+\_\+x86\+\_\+64.\+cpp 的引用(Include)关系图\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=233pt]{d4/d23/memcpy__x86__64_8cpp__incl}
\end{center}
\end{figure}
\doxysubsection*{函数}
\begin{DoxyCompactItemize}
\item 
static void $\ast$ \mbox{\hyperlink{memcpy__x86__64_8cpp_a11ebdd343428c466f2f35c376d7db6d0}{inline\+\_\+memcpy}} (void $\ast$\+\_\+\+\_\+restrict dst\+\_\+, const void $\ast$\+\_\+\+\_\+restrict src\+\_\+, size\+\_\+t size)
\item 
void $\ast$ \mbox{\hyperlink{memcpy__x86__64_8cpp_a156c5821f16ed93be8c7d315303f7b2a}{memcpy}} (void $\ast$\+\_\+\+\_\+restrict dst, const void $\ast$\+\_\+\+\_\+restrict src, size\+\_\+t size)
\end{DoxyCompactItemize}


\doxysubsection{函数说明}
\mbox{\Hypertarget{memcpy__x86__64_8cpp_a11ebdd343428c466f2f35c376d7db6d0}\label{memcpy__x86__64_8cpp_a11ebdd343428c466f2f35c376d7db6d0}} 
\index{memcpy\_x86\_64.cpp@{memcpy\_x86\_64.cpp}!inline\_memcpy@{inline\_memcpy}}
\index{inline\_memcpy@{inline\_memcpy}!memcpy\_x86\_64.cpp@{memcpy\_x86\_64.cpp}}
\doxysubsubsection{\texorpdfstring{inline\_memcpy()}{inline\_memcpy()}}
{\footnotesize\ttfamily static void $\ast$ inline\+\_\+memcpy (\begin{DoxyParamCaption}\item[{void $\ast$\+\_\+\+\_\+restrict}]{dst\+\_\+,  }\item[{const void $\ast$\+\_\+\+\_\+restrict}]{src\+\_\+,  }\item[{size\+\_\+t}]{size }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [static]}}

Custom memcpy implementation for Click\+House. It has the following benefits over using glibc\textquotesingle{}s implementation\+:
\begin{DoxyEnumerate}
\item Avoiding dependency on specific version of glibc\textquotesingle{}s symbol, like memcpy@GLIBC\+\_\+2.\+14 for portability.
\item Avoiding indirect call via PLT due to shared linking, that can be less efficient.
\item It\textquotesingle{}s possible to include this header and call inline\+\_\+memcpy directly for better inlining or interprocedural analysis.
\item Better results on our performance tests on current CPUs\+: up to 25\% on some queries and up to 0.\+7\%..1\% in average across all queries.
\end{DoxyEnumerate}

Writing our own memcpy is extremely difficult for the following reasons\+:
\begin{DoxyEnumerate}
\item The optimal variant depends on the specific CPU model.
\item The optimal variant depends on the distribution of size arguments.
\item It depends on the number of threads copying data concurrently.
\item It also depends on how the calling code is using the copied data and how the different memcpy calls are related to each other. Due to vast range of scenarios it makes proper testing especially difficult. When writing our own memcpy there is a risk to overoptimize it on non-\/representative microbenchmarks while making real-\/world use cases actually worse.
\end{DoxyEnumerate}

Most of the benchmarks for memcpy on the internet are wrong.

Let\textquotesingle{}s look at the details\+:

For small size, the order of branches in code is important. There are variants with specific order of branches (like here or in glibc) or with jump table (in asm code see example from Cosmopolitan libc\+: \href{https://github.com/jart/cosmopolitan/blob/de09bec215675e9b0beb722df89c6f794da74f3f/libc/nexgen32e/memcpy.S\#L61}{\texttt{ https\+://github.\+com/jart/cosmopolitan/blob/de09bec215675e9b0beb722df89c6f794da74f3f/libc/nexgen32e/memcpy.\+S\#\+L61}}) or with Duff device in C (see \href{https://github.com/skywind3000/FastMemcpy/}{\texttt{ https\+://github.\+com/skywind3000/\+Fast\+Memcpy/}})

It\textquotesingle{}s also important how to copy uneven sizes. Almost every implementation, including this, is using two overlapping movs.

It is important to disable -\/ftree-\/loop-\/distribute-\/patterns when compiling memcpy implementation, otherwise the compiler can replace internal loops to a call to memcpy that will lead to infinite recursion.

For larger sizes it\textquotesingle{}s important to choose the instructions used\+:
\begin{DoxyItemize}
\item SSE or AVX or AVX-\/512;
\item rep movsb; Performance will depend on the size threshold, on the CPU model, on the \char`\"{}erms\char`\"{} flag (\char`\"{}\+Enhansed Rep Mov\+S\char`\"{} -\/ it indicates that performance of \char`\"{}rep movsb\char`\"{} is decent for large sizes) \href{https://stackoverflow.com/questions/43343231/enhanced-rep-movsb-for-memcpy}{\texttt{ https\+://stackoverflow.\+com/questions/43343231/enhanced-\/rep-\/movsb-\/for-\/memcpy}}
\end{DoxyItemize}

Using AVX-\/512 can be bad due to throttling. Using AVX can be bad if most code is using SSE due to switching penalty (it also depends on the usage of \char`\"{}vzeroupper\char`\"{} instruction). But in some cases AVX gives a win.

It also depends on how many times the loop will be unrolled. We are unrolling the loop 8 times (by the number of available registers), but it not always the best.

It also depends on the usage of aligned or unaligned loads/stores. We are using unaligned loads and aligned stores.

It also depends on the usage of prefetch instructions. It makes sense on some Intel CPUs but can slow down performance on AMD. Setting up correct offset for prefetching is non-\/obvious.

Non-\/temporary (cache bypassing) stores can be used for very large sizes (more than a half of L3 cache). But the exact threshold is unclear -\/ when doing memcpy from multiple threads the optimal threshold can be lower, because L3 cache is shared (and L2 cache is partially shared).

Very large size of memcpy typically indicates suboptimal (not cache friendly) algorithms in code or unrealistic scenarios, so we don\textquotesingle{}t pay attention to using non-\/temporary stores.

On recent Intel CPUs, the presence of \char`\"{}erms\char`\"{} makes \char`\"{}rep movsb\char`\"{} the most benefitial, even comparing to non-\/temporary aligned unrolled stores even with the most wide registers.

memcpy can be written in asm, C or C++. The latter can also use inline asm. The asm implementation can be better to make sure that compiler won\textquotesingle{}t make the code worse, to ensure the order of branches, the code layout, the usage of all required registers. But if it is located in separate translation unit, inlining will not be possible (inline asm can be used to overcome this limitation). Sometimes C or C++ code can be further optimized by compiler. For example, clang is capable replacing SSE intrinsics to AVX code if -\/mavx is used.

Please note that compiler can replace plain code to memcpy and vice versa.
\begin{DoxyItemize}
\item memcpy with compile-\/time known small size is replaced to simple instructions without a call to memcpy; it is controlled by -\/fbuiltin-\/memcpy and can be manually ensured by calling \+\_\+\+\_\+builtin\+\_\+memcpy. This is often used to implement unaligned load/store without undefined behaviour in C++.
\item a loop with copying bytes can be recognized and replaced by a call to memcpy; it is controlled by -\/ftree-\/loop-\/distribute-\/patterns.
\item also note that a loop with copying bytes can be unrolled, peeled and vectorized that will give you inline code somewhat similar to a decent implementation of memcpy.
\end{DoxyItemize}

This description is up to date as of Mar 2021.

How to test the memcpy implementation for performance\+:
\begin{DoxyEnumerate}
\item Test on real production workload.
\item For synthetic test, see utils/memcpy-\/bench, but make sure you will do the best to exhaust the wide range of scenarios.
\end{DoxyEnumerate}

TODO\+: Add self-\/tuning memcpy with bayesian bandits algorithm for large sizes. See \href{https://habr.com/en/company/yandex/blog/457612/}{\texttt{ https\+://habr.\+com/en/company/yandex/blog/457612/}} We will use pointer arithmetic, so char pointer will be used. Note that \+\_\+\+\_\+restrict makes sense (otherwise compiler will reload data from memory instead of using the value of registers due to possible aliasing).

Standard memcpy returns the original value of dst. It is rarely used but we have to do it. If you use memcpy with small but non-\/constant sizes, you can call inline\+\_\+memcpy directly for inlining and removing this single instruction.

Small sizes and tails after the loop for large sizes. The order of branches is important but in fact the optimal order depends on the distribution of sizes in your application. This order of branches is from the disassembly of glibc\textquotesingle{}s code. We copy chunks of possibly uneven size with two overlapping movs. Example\+: to copy 5 bytes \mbox{[}0, 1, 2, 3, 4\mbox{]} we will copy tail \mbox{[}1, 2, 3, 4\mbox{]} first and then head \mbox{[}0, 1, 2, 3\mbox{]}.

Chunks of 8..16 bytes.

Chunks of 4..7 bytes.

Chunks of 2..3 bytes.

A single byte.

No bytes remaining.

Medium and large sizes.

Medium size, not enough for full loop unrolling.

We will copy the last 16 bytes.

Then we will copy every 16 bytes from the beginning in a loop. The last loop iteration will possibly overwrite some part of already copied last 16 bytes. This is Ok, similar to the code for small sizes above.

Large size with fully unrolled loop.

Align destination to 16 bytes boundary.

If not aligned -\/ we will copy first 16 bytes with unaligned stores.

Aligned unrolled copy. We will use half of available SSE registers. It\textquotesingle{}s not possible to have both src and dst aligned. So, we will use aligned stores and unaligned loads.

The latest remaining 0..127 bytes will be processed as usual.这是这个函数的调用关系图\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=252pt]{df/d07/memcpy__x86__64_8cpp_a11ebdd343428c466f2f35c376d7db6d0_icgraph}
\end{center}
\end{figure}
\mbox{\Hypertarget{memcpy__x86__64_8cpp_a156c5821f16ed93be8c7d315303f7b2a}\label{memcpy__x86__64_8cpp_a156c5821f16ed93be8c7d315303f7b2a}} 
\index{memcpy\_x86\_64.cpp@{memcpy\_x86\_64.cpp}!memcpy@{memcpy}}
\index{memcpy@{memcpy}!memcpy\_x86\_64.cpp@{memcpy\_x86\_64.cpp}}
\doxysubsubsection{\texorpdfstring{memcpy()}{memcpy()}}
{\footnotesize\ttfamily void $\ast$ memcpy (\begin{DoxyParamCaption}\item[{void $\ast$\+\_\+\+\_\+restrict}]{dst,  }\item[{const void $\ast$\+\_\+\+\_\+restrict}]{src,  }\item[{size\+\_\+t}]{size }\end{DoxyParamCaption})}

函数调用图\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=252pt]{df/d07/memcpy__x86__64_8cpp_a156c5821f16ed93be8c7d315303f7b2a_cgraph}
\end{center}
\end{figure}
